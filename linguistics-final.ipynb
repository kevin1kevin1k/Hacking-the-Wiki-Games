{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sys\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import lxml\n",
    "from urllib.parse import unquote\n",
    "import numpy as np\n",
    "import editdistance\n",
    "from nltk.corpus import wordnet as wn\n",
    "from gensim.models.wrappers.fasttext import FastText\n",
    "\n",
    "model = FastText.load_fasttext_format('./wiki.en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_prefix = 'https://en.wikipedia.org/wiki/'\n",
    "title_ignores = ['File:', 'Template:', 'Template_talk:', 'Special:', 'Wikipedia:', 'Portal:', 'Help:',\n",
    "           'Category:', 'Main_Page', 'Talk:', 'wiktionary', 'Template talk:']\n",
    "url_ignores = ['wiktionary', 'action=edit', 'wikimedia', 'File:', 'wikidata', 'wikibooks', 'http']\n",
    "\n",
    "def path_sim(w1, w2):\n",
    "    s1 = wn.synsets(w1)[0]\n",
    "    s2 = wn.synsets(w2)[0]\n",
    "    return s1.path_similarity(s2)\n",
    "\n",
    "similarity_dict = {\n",
    "    'fasttext': model.similarity,\n",
    "    'wordnet': path_sim,\n",
    "}\n",
    "\n",
    "present_dict = {\n",
    "    'fasttext': lambda w: w in model,\n",
    "    'wordnet': lambda w: len(wn.synsets(w)) > 0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titles2hrefs(target):\n",
    "    url = wiki_prefix + target\n",
    "    res = requests.get(url)\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\n",
    "    body = soup.find('div', {'class': 'mw-parser-output'})\n",
    "    links = body.findAll('a', title=True)\n",
    "    titles2hrefs = dict()\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "        title = link.get('title')\n",
    "        if any([i in href for i in url_ignores]):\n",
    "            continue\n",
    "        if any([i in title for i in title_ignores]):\n",
    "            continue\n",
    "        title = unquote(title).lower()\n",
    "        if title not in titles2hrefs:\n",
    "            titles2hrefs[title] = unquote(href[len('/wiki/'):])\n",
    "    return titles2hrefs\n",
    "\n",
    "def get_hrefs(target):\n",
    "    return list(get_titles2hrefs(target).values())\n",
    "\n",
    "def run_one(start, target, similarity, top):\n",
    "    assert top >= 1\n",
    "    present = present_dict[similarity]\n",
    "    now = start\n",
    "    path = []\n",
    "    path.append(now)\n",
    "    target_s = re.split(',|:|[0-9]|\\.| |_|\\(|\\)', target)\n",
    "    while target not in [w.lower() for w in [now, '_'.join(now.split(' ')), ' '.join(now.split('_'))]]:\n",
    "        hrefs = get_hrefs(now)\n",
    "        hrefs_good = []\n",
    "        sims = []\n",
    "        for href in hrefs:\n",
    "            href_s = re.split(',|:|[0-9]|\\.| |_|\\(|\\)', href.lower())\n",
    "            sim = 0.0\n",
    "            href_s = [t for t in href_s if t != '' and present(t)]\n",
    "            cnt = len(href_s)\n",
    "            similarities = [similarity_dict[similarity](target_tok, href_tok)\n",
    "                            for href_tok in href_s for target_tok in target_s]\n",
    "            similarities = [s for s in similarities if s is not None]\n",
    "            if len(similarities) > 0 and href not in path:\n",
    "                sim = np.average(similarities)\n",
    "                hrefs_good.append(href)\n",
    "                sims.append(sim)\n",
    "        argsort = np.argsort(sims)[::-1]\n",
    "        i = 0\n",
    "        sim1 = []\n",
    "        while sims[argsort[i]] == 1:\n",
    "            sim1.append(hrefs_good[argsort[i]])\n",
    "            i += 1\n",
    "        eds = [editdistance.eval(target, s) for s in sim1]\n",
    "        argsort[:len(sim1)] = [argsort[:len(sim1)][i] for i in np.argsort(eds)]\n",
    "        topK = argsort[:top]\n",
    "\n",
    "        if top > 1:\n",
    "            print([hrefs_good[i] for i in topK])\n",
    "        idx = int(input()) if top > 1 else 0\n",
    "        assert idx in [0, 1, 2]\n",
    "        now = hrefs_good[topK[idx]]\n",
    "        print(unquote(now))\n",
    "        path.append(now)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(start, target, similarity, top=1, middle=None):\n",
    "    if middle is None:\n",
    "        path = run_one(start, target, similarity, top)\n",
    "        print('Done. Length = {}'.format(len(path)))\n",
    "        return path\n",
    "    else:\n",
    "        middle = middle.lower()\n",
    "        path1 = run_one(start, middle, similarity, top)\n",
    "        path2 = run_one(middle, target, similarity, top)\n",
    "        print('Done. Length = {}'.format(len(path1) + len(path2)))\n",
    "        return path1 + path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squirrel\n",
      "Mouse-like_hamster\n",
      "Hamster\n",
      "Roborovski_hamster\n",
      "Rodent\n",
      "Armadillo\n",
      "Chipmunk\n",
      "Red-tailed_chipmunk\n",
      "Antelope_squirrel\n",
      "Gray-footed_chipmunk\n",
      "Townsend's_chipmunk\n",
      "Lodgepole_chipmunk\n",
      "Mammal\n",
      "Rabbit\n",
      "Rabbit_rabbit_rabbit\n",
      "Three_hares\n",
      "Rabbit_rabbit\n",
      "Maneki-neko\n",
      "Meowth\n",
      "Blastoise\n",
      "Greninja\n",
      "Sonic_the_Hedgehog_(character)\n",
      "Hedgehog\n",
      "Done. Length = 24\n",
      "\n",
      "=====\n",
      "Supreme_Court_of_the_United_States\n",
      "United_States\n",
      "Helianthus\n",
      "Helianthus_decapetalus\n",
      "Muskrat\n",
      "Mouse\n",
      "Hamster\n",
      "Mouse-like_hamster\n",
      "Squirrel\n",
      "Dormouse\n",
      "Rodent\n",
      "Armadillo\n",
      "Chipmunk\n",
      "Red-tailed_chipmunk\n",
      "Antelope_squirrel\n",
      "Gray-footed_chipmunk\n",
      "Townsend's_chipmunk\n",
      "Lodgepole_chipmunk\n",
      "Mammal\n",
      "Rabbit\n",
      "Rabbit_rabbit_rabbit\n",
      "Three_hares\n",
      "Rabbit_rabbit\n",
      "Maneki-neko\n",
      "Meowth\n",
      "Blastoise\n",
      "Greninja\n",
      "Sonic_the_Hedgehog_(character)\n",
      "Hedgehog\n",
      "Done. Length = 31\n",
      "\n",
      "=====\n",
      "Squirrel\n",
      "New_World_porcupine\n",
      "Andean_porcupine\n",
      "Bahia_porcupine\n",
      "Rothschild's_porcupine\n",
      "Stump-tailed_porcupine\n",
      "Bicolored-spined_porcupine\n",
      "Prehensile-tailed_porcupine\n",
      "Black-tailed_hairy_dwarf_porcupine\n",
      "Porcupine\n",
      "Hedgehog\n",
      "Done. Length = 12\n",
      "\n",
      "=====\n",
      "Supreme_Court_of_the_United_States\n",
      "United_States\n",
      "Turkey_(bird)\n",
      "Elephant_bird\n",
      "Lemur\n",
      "Rodent\n",
      "Porcupine\n",
      "Hedgehog\n",
      "Done. Length = 10\n",
      "\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "start = 'Fruit'\n",
    "target = 'Hedgehog'.lower()\n",
    "\n",
    "for similarity in ['fasttext', 'wordnet']:\n",
    "    for middle in [None, 'United_States']:\n",
    "        run(start, target, similarity=similarity, top=1, middle=middle)\n",
    "        print('\\n=====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wayback_Machine\n",
      "WABAC_machine\n",
      "ENIAC\n",
      "Adding_machine\n",
      "Electromechanical\n",
      "Enigma_machine\n",
      "C-36_(cipher_machine)\n",
      "C-52_(cipher_machine)\n",
      "Rotor_machine\n",
      "Hebern_Rotor_Machine\n",
      "Enigma_(machine)\n",
      "Hebern_rotor_machine\n",
      "Combined_Cipher_Machine\n",
      "JADE_(cipher_machine)\n",
      "Purple_(cipher_machine)\n",
      "Enigma_Machine\n",
      "NEMA_(machine)\n",
      "Red_(cipher_machine)\n",
      "Japanese_M-1_cipher_machine\n",
      "International_Standard_Book_Number\n",
      "COBOL\n",
      "Mainframe_computer\n",
      "Virtual_machine\n",
      "Machine_learning\n",
      "Done. Length = 25\n",
      "\n",
      "=====\n",
      "Supreme_Court_of_the_United_States\n",
      "United_States\n",
      "Machine_tool\n",
      "Machine\n",
      "Turing_machine\n",
      "Postâ€“Turing_machine\n",
      "Computation\n",
      "Computer\n",
      "Machine_learning\n",
      "Done. Length = 11\n",
      "\n",
      "=====\n",
      "Wayback_Machine\n",
      "WABAC_machine\n",
      "Plot_device\n",
      "Stylistic_device\n",
      "Rhetorical_device\n",
      "Translation_(rhetorical_device)\n",
      "Scientific_method\n",
      "Inquiry-based_learning\n",
      "Project-based_learning\n",
      "Problem-based_learning\n",
      "Phenomenon-based_learning\n",
      "Deeper_learning\n",
      "Hands-on_learning\n",
      "Learning\n",
      "Augmented_learning\n",
      "Observational_learning\n",
      "Machine_learning\n",
      "Done. Length = 18\n",
      "\n",
      "=====\n",
      "Supreme_Court_of_the_United_States\n",
      "United_States\n",
      "Wayback_Machine\n",
      "WABAC_machine\n",
      "Plot_device\n",
      "Stylistic_device\n",
      "Rhetorical_device\n",
      "Translation_(rhetorical_device)\n",
      "Scientific_method\n",
      "Inquiry-based_learning\n",
      "Project-based_learning\n",
      "Problem-based_learning\n",
      "Phenomenon-based_learning\n",
      "Deeper_learning\n",
      "Hands-on_learning\n",
      "Learning\n",
      "Augmented_learning\n",
      "Observational_learning\n",
      "Machine_learning\n",
      "Done. Length = 21\n",
      "\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "start = 'Fruit'\n",
    "target = 'Machine_learning'.lower()\n",
    "\n",
    "for similarity in ['fasttext', 'wordnet']:\n",
    "    for middle in [None, 'United_States']:\n",
    "        run(start, target, similarity=similarity, top=1, middle=middle)\n",
    "        print('\\n=====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def traverse(connect, current, lst):\n",
    "    if current[-1] == len(lst)-1:\n",
    "#         print(current)\n",
    "        print(np.array(lst)[current])\n",
    "        return\n",
    "    next_ = list(current)\n",
    "    next_.append(None)\n",
    "    for i in connect[current[-1]]:\n",
    "        next_[-1] = i\n",
    "        traverse(connect, next_, lst)\n",
    "\n",
    "def sublst(lst):\n",
    "    connect = []\n",
    "    for i in range(len(lst)):\n",
    "        url = wiki_prefix + lst[i]\n",
    "        res = requests.get(url)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        body = soup.find('div', {'class': 'mw-parser-output'})\n",
    "        reach = []\n",
    "        for j in range(i+1, len(lst)):\n",
    "            if body.find('a', {'href': '/wiki/' + lst[j]}) is not None:\n",
    "                reach.append(j)\n",
    "        connect.append(reach)\n",
    "#     print(connect)\n",
    "    traverse(connect, [0], lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wayback_Machine\n",
      "WABAC_machine\n",
      "Plot_device\n",
      "Stylistic_device\n",
      "Rhetorical_device\n",
      "Translation_(rhetorical_device)\n",
      "Scientific_method\n",
      "Inquiry-based_learning\n",
      "Project-based_learning\n",
      "Problem-based_learning\n",
      "Phenomenon-based_learning\n",
      "Deeper_learning\n",
      "Hands-on_learning\n",
      "Learning\n",
      "Augmented_learning\n",
      "Observational_learning\n",
      "Machine_learning\n",
      "Done. Length = 18\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Project-based_learning'\n",
      " 'Problem-based_learning' 'Phenomenon-based_learning' 'Deeper_learning'\n",
      " 'Hands-on_learning' 'Learning' 'Augmented_learning'\n",
      " 'Observational_learning' 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Project-based_learning'\n",
      " 'Problem-based_learning' 'Phenomenon-based_learning' 'Deeper_learning'\n",
      " 'Hands-on_learning' 'Learning' 'Observational_learning'\n",
      " 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Project-based_learning'\n",
      " 'Problem-based_learning' 'Phenomenon-based_learning' 'Deeper_learning'\n",
      " 'Hands-on_learning' 'Learning' 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Project-based_learning'\n",
      " 'Problem-based_learning' 'Phenomenon-based_learning' 'Learning'\n",
      " 'Augmented_learning' 'Observational_learning' 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Project-based_learning'\n",
      " 'Problem-based_learning' 'Phenomenon-based_learning' 'Learning'\n",
      " 'Observational_learning' 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Project-based_learning'\n",
      " 'Problem-based_learning' 'Phenomenon-based_learning' 'Learning'\n",
      " 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Project-based_learning'\n",
      " 'Phenomenon-based_learning' 'Deeper_learning' 'Hands-on_learning'\n",
      " 'Learning' 'Augmented_learning' 'Observational_learning'\n",
      " 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Project-based_learning'\n",
      " 'Phenomenon-based_learning' 'Deeper_learning' 'Hands-on_learning'\n",
      " 'Learning' 'Observational_learning' 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Project-based_learning'\n",
      " 'Phenomenon-based_learning' 'Deeper_learning' 'Hands-on_learning'\n",
      " 'Learning' 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Project-based_learning'\n",
      " 'Phenomenon-based_learning' 'Learning' 'Augmented_learning'\n",
      " 'Observational_learning' 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Project-based_learning'\n",
      " 'Phenomenon-based_learning' 'Learning' 'Observational_learning'\n",
      " 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Project-based_learning'\n",
      " 'Phenomenon-based_learning' 'Learning' 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Problem-based_learning'\n",
      " 'Phenomenon-based_learning' 'Deeper_learning' 'Hands-on_learning'\n",
      " 'Learning' 'Augmented_learning' 'Observational_learning'\n",
      " 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Problem-based_learning'\n",
      " 'Phenomenon-based_learning' 'Deeper_learning' 'Hands-on_learning'\n",
      " 'Learning' 'Observational_learning' 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Problem-based_learning'\n",
      " 'Phenomenon-based_learning' 'Deeper_learning' 'Hands-on_learning'\n",
      " 'Learning' 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Problem-based_learning'\n",
      " 'Phenomenon-based_learning' 'Learning' 'Augmented_learning'\n",
      " 'Observational_learning' 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Problem-based_learning'\n",
      " 'Phenomenon-based_learning' 'Learning' 'Observational_learning'\n",
      " 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Problem-based_learning'\n",
      " 'Phenomenon-based_learning' 'Learning' 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Phenomenon-based_learning'\n",
      " 'Deeper_learning' 'Hands-on_learning' 'Learning' 'Augmented_learning'\n",
      " 'Observational_learning' 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Phenomenon-based_learning'\n",
      " 'Deeper_learning' 'Hands-on_learning' 'Learning' 'Observational_learning'\n",
      " 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Phenomenon-based_learning'\n",
      " 'Deeper_learning' 'Hands-on_learning' 'Learning' 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Phenomenon-based_learning'\n",
      " 'Learning' 'Augmented_learning' 'Observational_learning'\n",
      " 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Phenomenon-based_learning'\n",
      " 'Learning' 'Observational_learning' 'Machine_learning']\n",
      "['Fruit' 'Wayback_Machine' 'WABAC_machine' 'Plot_device'\n",
      " 'Stylistic_device' 'Rhetorical_device' 'Translation_(rhetorical_device)'\n",
      " 'Scientific_method' 'Inquiry-based_learning' 'Phenomenon-based_learning'\n",
      " 'Learning' 'Machine_learning']\n"
     ]
    }
   ],
   "source": [
    "path = run(start, target, similarity='wordnet', top=1)\n",
    "sublst(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wn_path(w1, w2):\n",
    "    s1 = wn.synsets(w1)[0]\n",
    "    s2 = wn.synsets(w2)[0]\n",
    "    lch = s1.lowest_common_hypernyms(s2)[0]\n",
    "    \n",
    "    path1 = s1.hypernym_paths()[0]\n",
    "    path2 = s2.hypernym_paths()[0]\n",
    "    \n",
    "    i1 = path1.index(lch)\n",
    "    i2 = path2.index(lch)\n",
    "    \n",
    "    return list(reversed(path2[i2:])) + path1[i1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrefs = get_hrefs('fruit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "porcupine\n",
      "rodent\n",
      "placental\n",
      "mammal\n",
      "vertebrate\n",
      "chordate\n",
      "animal\n",
      "--- Animal\n",
      "--- Animal-free_agriculture\n",
      "--- Animal_rights\n",
      "--- Animal_welfare\n",
      "--- Eating_Animals\n",
      "organism\n",
      "living_thing\n",
      "whole\n",
      "whole\n",
      "natural_object\n",
      "plant_part\n",
      "plant_organ\n",
      "reproductive_structure\n",
      "fruit\n",
      "--- Fruit_(disambiguation)\n",
      "--- List_of_culinary_fruits\n",
      "--- Orange_(fruit)\n",
      "--- Nut_(fruit)\n",
      "--- Fruit_anatomy\n",
      "--- Accessory_fruit\n",
      "--- Aggregate_fruit\n",
      "--- Capsule_(fruit)\n",
      "--- Follicle_(fruit)\n",
      "--- Samara_(fruit)\n",
      "--- Utricle_(fruit)\n",
      "--- Multiple_fruit\n",
      "--- Breadfruit\n",
      "--- Kiwifruit\n",
      "--- Grapefruit\n",
      "--- Lime_(fruit)\n",
      "--- Squash_(fruit)\n",
      "--- Seedless_fruit\n",
      "--- Fruit_beer\n",
      "--- Fruit_Basket\n",
      "--- Fruit_Bouquet\n",
      "--- Ethylene-ripened_fruits\n",
      "--- Fruit_tree\n",
      "--- Fruitarianism\n",
      "--- List_of_fruit_dishes\n",
      "--- Fruit#Simple_fruit\n",
      "--- Compound_fruit\n"
     ]
    }
   ],
   "source": [
    "for synset in wn_path('fruit', 'hedgehog'):\n",
    "    print(synset._name.split('.')[0])\n",
    "    names = synset._name.split('.')[0].split('_')\n",
    "    for href in hrefs:\n",
    "        if all(name in href.lower() for name in names):\n",
    "            print('---', href)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rabbit\n",
      "Rabbit_rabbit_rabbit\n",
      "Three_hares\n",
      "Rabbit_rabbit\n",
      "Maneki-neko\n",
      "Meowth\n",
      "Blastoise\n",
      "Greninja\n",
      "Sonic_the_Hedgehog_(character)\n",
      "Hedgehog\n",
      "Done. Length = 11\n",
      "Mammal\n",
      "Porcupine\n",
      "Hedgehog\n",
      "Done. Length = 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['animal', 'Mammal', 'Porcupine', 'Hedgehog']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run('animal', 'hedgehog', similarity='fasttext', top=1)\n",
    "run('animal', 'hedgehog', similarity='wordnet', top=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
